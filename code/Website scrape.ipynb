{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load current ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assumes excel file is already created. For project owners only, rest of notebook will not run otherwise. \n",
    "#contact J.Rosen.1392@gmail.com\n",
    "current_ideas = pd.read_excel('../assets/FULL DB VALUES.xlsx')\n",
    "current_ideas = current_ideas['Idea URL'].tolist()\n",
    "current_ideas = [idea.split('.com',1)[1] for idea in current_ideas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import credentials\n",
    "user = credentials.user\n",
    "passw = credentials.passw\n",
    "\n",
    "login_url = \"https://valueinvestorsclub.com/login\"\n",
    "\n",
    "#persist login through all requests\n",
    "session_requests = requests.session()\n",
    "\n",
    "#get login csrf token\n",
    "result = session_requests.get(login_url)\n",
    "tree = html.fromstring(result.text)\n",
    "authenticity_token = list(set(tree.xpath(\"//input[@name='_token']/@value\")))[0]\n",
    "    \n",
    "#create payload\n",
    "payload = {\n",
    "    \"login[login_name]\": user, \n",
    "    \"login[password]\": passw, \n",
    "    \"_token\": authenticity_token,\n",
    "    \"commit\": \"Login\"\n",
    "    }\n",
    "\n",
    "#perform login\n",
    "result = session_requests.post(login_url, data = payload, headers = dict(referer = login_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    html = session_requests.get(url, headers = dict(referer = url)).text\n",
    "    return BeautifulSoup(html,'lxml')\n",
    "\n",
    "def get_ideas(url):\n",
    "    '''pulls href text from each \"full idea page\"'''\n",
    "    soup = make_soup(url)\n",
    "    idea_links = [link.a['href'] for link in soup('span','vich1')]\n",
    "    return idea_links\n",
    "\n",
    "def get_small_info(url):\n",
    "    '''pulls individual write-up features'''\n",
    "    soup = make_soup(url)\n",
    "    \n",
    "    idea_name = soup.find('span','idea_company_name').text\n",
    "    try:\n",
    "        ticker = soup.find('span',{'style':'color:#ccc;'}).text\n",
    "    except:\n",
    "        ticker = np.nan\n",
    "    member_link = soup.find('a', {'class':'display_name'})['href']\n",
    "    member_name = soup.find('a', {'class':'display_name'})['title']\n",
    "    try:\n",
    "        quality = float(soup.find('span', {'id':'ratings_q'})['data-rateit-value'])\n",
    "    except:\n",
    "        quality = np.nan\n",
    "    try:\n",
    "        performance = float(soup.find('span', {'id':'ratings_p'})['data-rateit-value'])\n",
    "    except:\n",
    "        performance = np.nan\n",
    "    description_raw = soup.find('div',{'id':'description'}).text\n",
    "    date_text = soup.find('div', {'style':'display:inline-block;'}).text\n",
    "    submission_date = datetime.strptime(date_text.split(' by')[0].replace(',','').replace(' ',''),'%B%d%Y')\n",
    "    try:\n",
    "        if soup.find('span', {'class':'label label-short'}).string == \"S\":\n",
    "            is_long = False\n",
    "    except Exception as e:\n",
    "        is_long = True\n",
    "    \n",
    "    return pd.DataFrame({'Idea': [idea_name],\n",
    "            'Idea URL': [url],\n",
    "            'Ticker': [ticker],\n",
    "            'Author': [member_name],\n",
    "            'Author link': [member_link],\n",
    "            'Submission Date': [submission_date],\n",
    "            'Quality': [quality],\n",
    "            'Performance': [performance],\n",
    "            'Description': [description_raw],\n",
    "            'Long': [is_long]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = 'https://valueinvestorsclub.com'\n",
    "add_ons = ['0-9','A-C','D-F','G-J','K-N','O-R','S-V','W-Z']    \n",
    "\n",
    "#compile A-Z into one list\n",
    "idea_pages = []\n",
    "for add_on in add_ons:\n",
    "    time.sleep(1)\n",
    "    idea_pages.extend(get_ideas(base_url+'/ideas/atoz/'+add_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/idea/FERROGLOBE_PLC/139399\n",
      "/idea/MELLANOX_TECHNOLOGIES_LTD/139398\n",
      "/idea/Sirius_Real_Estate/139396\n"
     ]
    }
   ],
   "source": [
    "#grab each idea's features and compile into dataframe\n",
    "data_frames = []\n",
    "for idea in idea_pages:\n",
    "    try:\n",
    "        if idea not in current_ideas and idea != '/idea/Celebrate_Express/2693':\n",
    "            print idea\n",
    "            time.sleep(1.3) #this may take a while, but always be nice\n",
    "            data_frames.append(get_small_info(base_url+idea))\n",
    "            df = pd.concat(data_frames, ignore_index = True)\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write to excel file for future steps\n",
    "df.to_excel('../assets/Site Scrape '+datetime.strftime(datetime.now(),'%Y%m%d')+'.xlsx', sheet_name='Sheet1', engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
